{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed93627e",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14018c",
   "metadata": {},
   "source": [
    "==>>Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications\n",
    "\n",
    "Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.\n",
    "\n",
    "What is Web Scraping used for?\n",
    "\n",
    "1. Price Monitoring:\n",
    "\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies.\n",
    "\n",
    "2. Market Research:\n",
    "\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "5. Email Marketing:\n",
    "\n",
    "Companies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c92b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d20a928",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25fc38",
   "metadata": {},
   "source": [
    "==>Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers.\n",
    "\n",
    "1)Browser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers.\n",
    "\n",
    "2)Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that you buy the scraper from. These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe506b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d26c8b",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1e4c8",
   "metadata": {},
   "source": [
    "==>>Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410edcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aada27e",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95a4c0",
   "metadata": {},
   "source": [
    "==>Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365e82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d16dd44",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cd33a",
   "metadata": {},
   "source": [
    "==>1. Amazon EC2\n",
    "\n",
    "You don’t have to invest in costly physical services. Instead, you can create virtual machines with Amazon EC2 while managing other server features such as ports, security, and storage. Spend less time maintaining your servers and more time on your strategic projects. Invariably, Amazon EC2 is one of the most popular and fastest-growing of the many AWS services.\n",
    "\n",
    "2. Amazon RDS\n",
    "\n",
    "The Amazon Relational Database Service (RDS) was designed to make your infrastructure more user friendly. By using this AWS service, you can create dedicated instances of databases within minutes. Not to mention, these instances can support multiple database engines including SQL Server, SQL, PostgreSQL, and more. Take your time back and stop spending hours maintaining your database servers. Let Amazon RDS do the work for you.\n",
    "\n",
    "3. Amazon CloudFront\n",
    "\n",
    "This service helps to improve website speed and access to cloud-based data. CloudFront works as a Global Content Delivery Service (CDN) to deliver content efficiently to end users. You’ll notice a significant increase in web page loading speed with this service. It even pulls website static files from data centers throughout the world.\n",
    "\n",
    "4. AWS Beanstalk\n",
    "\n",
    "AWS Beanstalk really is a timesaver. It automates the setup, configuration, and provisioning of other AWS services such as EC2, RDS, and S3. Not to mention, the automated setup also helps to mitigate human error.\n",
    "\n",
    "5. AWS Lambda\n",
    "\n",
    "When your server is inundated with an influx of requests, are you overwhelmed without knowing precisely how to respond? It’s quite possible your current server infrastructure cannot support the demands of your current speed of development. In this case, AWS Lambda is designed to support any load of development. You handle the coding, and AWS Lambda will offer the right amounts of support and required resources while scaling to ensure your systems are no longer stretched past capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9950d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
